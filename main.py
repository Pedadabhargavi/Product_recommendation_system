# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qUy4VhiM5mPGBgvXWUuydbgP4nIy4lJp

coneecting to drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""checking the presence of dataset"""

import os
for dirname, _, filenames in os.walk('/content/drive/MyDrive/mldata'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""installing necessary packeges"""

pip install surprise

"""importing necessary packages"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from collections import defaultdict
from surprise import KNNWithMeans
from surprise import SVD, SVDpp
from surprise import KNNBaseline
from surprise import KNNBasic
from surprise import KNNWithZScore
from surprise import BaselineOnly
from surprise import Dataset
from surprise import Reader
from surprise import accuracy
from surprise.model_selection import train_test_split
from surprise.model_selection import cross_validate
from surprise.model_selection import KFold
from surprise.model_selection import GridSearchCV
import time

"""loading the dataset"""

start_time = time.time()
df = pd.read_csv("/content/drive/MyDrive/mldata/ratings_Electronics.csv", names=["userId", "productId", "rating", "timestamp"])
df.head()

rows_count, columns_count = df.shape
print('Total Number of rows :', rows_count)
print('Total Number of columns :', columns_count)

unique_userId = df['userId'].nunique()
unique_productId = df['productId'].nunique()
print('Total number of unique Users    : ', unique_userId)
print('Total number of unique Products : ', unique_productId)

"""checking whether are there any missing values in dataset"""

df.apply(lambda x : sum(x.isnull()))

"""mathematical analysis of data"""

df_transpose = df.describe().T
df_transpose

df['rating'].value_counts()

rating_counts = pd.DataFrame(df['rating'].value_counts()).reset_index()
rating_counts.columns = ['Labels', 'Ratings']
rating_counts

df = df.drop(['timestamp'], axis=1)

df1 = df.copy()

df1.head()

users_counts = df1['userId'].value_counts().rename('users_counts')
users_data   = df1.merge(users_counts.to_frame(),
                                left_on='userId',
                                right_index=True)

subset_df = users_data[users_data.users_counts >= 50]
subset_df.head()

product_rating_counts = subset_df['productId'].value_counts().rename('product_rating_counts')
product_rating_data   = subset_df.merge(product_rating_counts.to_frame(),
                                left_on='productId',
                                right_index=True)

product_rating_data = product_rating_data[product_rating_data.product_rating_counts >= 10]
product_rating_data.head()

amazon_df = product_rating_data.copy()

panda_data = amazon_df.drop(['users_counts', 'product_rating_counts'], axis=1)

panda_data.head()

k = 5

reader = Reader(rating_scale=(1, 5))

surprise_data = Dataset.load_from_df(panda_data[['userId', 'productId', 'rating']], reader)

trainset, testset = train_test_split(surprise_data, test_size=.30, random_state=7)

panda_data.groupby('productId')['rating'].mean().head()

panda_data.groupby('productId')['rating'].mean().sort_values(ascending=False).head()

prod_rating_count = pd.DataFrame(panda_data.groupby('productId')['rating'].mean().sort_values(ascending=False))
prod_rating_count['prod_rating_count'] = pd.DataFrame(panda_data.groupby('productId')['rating'].count())
prod_rating_count.head(k)

basic_poplurity_model = prod_rating_count.sort_values(by=['prod_rating_count'], ascending=False)
basic_poplurity_model.head(k)

panda_data_grouped = panda_data.groupby('productId').agg({'userId': 'count'}).reset_index()
panda_data_grouped.rename(columns = {'userId': 'score'},inplace=True)
panda_data_grouped.head()

"""**Model1- Popularity based**"""

panda_data_sort = panda_data_grouped.sort_values(['score', 'productId'], ascending = [0,1])

#Generate a recommendation rank based upon score
panda_data_sort['Rank'] = panda_data_sort['score'].rank(ascending=0, method='first')

#Get the top 5 recommendations
popularity_recommendations = panda_data_sort.head(k)
popularity_recommendations

import warnings
warnings.filterwarnings('ignore')
def recommend(userId):
    user_recommendations = popularity_recommendations

    #Adding user_id column for which the recommendations are being generated
    user_recommendations['userID'] = userId

    #Bringing user_id column to the front
    cols = user_recommendations.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    user_recommendations = user_recommendations[cols]

    return user_recommendations

find_recom = [15,121,55,230,344]   # This list is user choice.
for i in find_recom:
    print("Here is the recommendation for the userId: %d\n" %(i))
    print(recommend(i))
    print("\n")

"""**model2-SVD(Singular value decomposition)**"""

cv_results = []

svd_param_grid = {'n_epochs': [20, 25], 'lr_all': [0.007, 0.009, 0.01], 'reg_all': [0.4, 0.6]}

svd_gs = GridSearchCV(SVD, svd_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)
svdpp_gs = GridSearchCV(SVDpp, svd_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)

svd_gs.fit(surprise_data)
svdpp_gs.fit(surprise_data)

# best RMSE score
print(svd_gs.best_score['rmse'])
print(svdpp_gs.best_score['rmse'])

# combination of parameters that gave the best RMSE score
print(svd_gs.best_params['rmse'])
print(svdpp_gs.best_params['rmse'])

start_time = time.time()

# Creating Model using best parameters
svd_model = SVD(n_epochs=20, lr_all=0.005, reg_all=0.2)

# Training the algorithm on the trainset
svd_model.fit(trainset)


# Predicting for test set
predictions_svd = svd_model.test(testset)

# Evaluating RMSE, MAE of algorithm SVD on 5 split(s) by cross validation
svd_cv = cross_validate(svd_model, surprise_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

# Storing Crossvalidation Results in dataframe
svd_df = pd.DataFrame.from_dict(svd_cv)
svd_described = svd_df.describe()
cv_results = pd.DataFrame([['SVD', svd_described['test_rmse']['mean'], svd_described['test_mae']['mean'],
                           svd_described['fit_time']['mean'], svd_described['test_time']['mean']]],
                            columns = ['Model', 'RMSE', 'MAE', 'Fit Time', 'Test Time'])


# get RMSE
print("\n\n==================== Model Evaluation ===============================")
accuracy.rmse(predictions_svd, verbose=True)
print("=====================================================================")
computational_time = time.time() - start_time
print('\n Computational Time : %0.3fs' %(computational_time))
cv_results

"""output of SVD model which gives top k (here we took k=5) recommendations for each userid"""

top_n = defaultdict(list)
def get_top_n(predictions, n=k):
    # First map the predictions to each user.
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    # Then sort the predictions for each user and retrieve the k highest ones.
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n

top_n = get_top_n(predictions_svd, n=k)
top_n

"""**Model3-SVD++**"""

start_time = time.time()

# Creating Model using best parameters
svdpp_model = SVDpp(n_epochs=25, lr_all=0.01, reg_all=0.4)

# Training the algorithm on the trainset
svdpp_model.fit(trainset)


# Predicting for test set
predictions_svdpp = svdpp_model.test(testset)

# Evaluating RMSE, MAE of algorithm SVDpp on 5 split(s) by cross validation
svdpp_cv = cross_validate(svdpp_model, surprise_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

# Storing Crossvalidation Results in dataframe
svdpp_df = pd.DataFrame.from_dict(svdpp_cv)
svdpp_described = svdpp_df.describe()
svdpp_cv_results = pd.DataFrame([['SVDpp', svdpp_described['test_rmse']['mean'], svdpp_described['test_mae']['mean'],
                           svdpp_described['fit_time']['mean'], svdpp_described['test_time']['mean']]],
                            columns = ['Model', 'RMSE', 'MAE', 'Fit Time', 'Test Time'])

cv_results = cv_results.append(svdpp_cv_results, ignore_index=True)

# get RMSE
print("\n\n==================== Model Evaluation ===============================")
accuracy.rmse(predictions_svdpp, verbose=True)
print("=====================================================================")
computational_time = time.time() - start_time
print('\n Computational Time : %0.3fs' %(computational_time))
cv_results

"""Output of SVD++ which gives recommendations for each user"""

top_n = defaultdict(list)
def get_top_n(predictions, n=k):
    # First map the predictions to each user.
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    # Then sort the predictions for each user and retrieve the k highest ones.
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n

top_n = get_top_n(predictions_svdpp, n=k)
top_n

import matplotlib.pyplot as plt
import numpy as np

# Sample data for RMSE and MAE values of different algorithms
algorithms = ['SVD', 'SVD++']
rmse_values = [0.874474, 0.872162]
mae_values = [0.646063, 0.645241]

# Set up the figure and axis
fig, ax = plt.subplots()

# Set the width of the bars
bar_width = 0.35

# Set the x locations for the groups
index = np.arange(len(algorithms))

# Plot RMSE values
bar1 = ax.bar(index, rmse_values, bar_width, label='RMSE')

# Plot MAE values
bar2 = ax.bar(index + bar_width, mae_values, bar_width, label='MAE')

# Add labels, title, and legend
ax.set_xlabel('Algorithms')
ax.set_ylabel('Metric Values')
ax.set_title('RMSE and MAE Values of Recommendation Algorithms')
ax.set_xticks(index + bar_width / 2)
ax.set_xticklabels(algorithms)
ax.legend()

# Display the plot
plt.show()